{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingSketchRnn import TrainingSketchRnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=0.001\n",
    "BATCH_SIZE=300\n",
    "N_EPOCHS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on GPU\n",
      "(3845, 2, 16, 9) SHAPE NUMPU\n",
      "3845 LEN DATASET\n",
      "2307 SELF TRAIN\n",
      "769 SELF validation\n",
      "769 SELF test\n",
      "run on GPU\n",
      "Train Epoch: 0 [   0/2307 ( 0%)]      Loss: 42599.883333\n",
      "bce: 105.232962, kld: 169.978594\n",
      "Train Epoch: 0 [1500/2307 (71%)]      Loss: 12328.354167\n",
      "bce: 104.120690, kld: 48.896934\n",
      "====> Epoch: 0 Average loss: 15580.9954, bce: 95.1859, kld: 61.9432\n",
      "====> Testing Average Loss: 80.83551588914175\n",
      "Train Epoch: 1 [   0/2307 ( 0%)]      Loss: 12311.910000\n",
      "bce: 103.895879, kld: 48.832057\n",
      "Train Epoch: 1 [1500/2307 (71%)]      Loss: 9930.810000\n",
      "bce: 103.211510, kld: 39.310394\n",
      "====> Epoch: 1 Average loss: 10075.4205, bce: 94.0859, kld: 39.9253\n",
      "====> Testing Average Loss: 80.1000284460338\n",
      "Train Epoch: 2 [   0/2307 ( 0%)]      Loss: 9483.995000\n",
      "bce: 102.876185, kld: 37.524476\n",
      "Train Epoch: 2 [1500/2307 (71%)]      Loss: 9257.291667\n",
      "bce: 102.246582, kld: 36.620179\n",
      "====> Epoch: 2 Average loss: 8671.7419, bce: 93.3078, kld: 34.3137\n",
      "====> Testing Average Loss: 79.50487900276333\n",
      "Train Epoch: 3 [   0/2307 ( 0%)]      Loss: 8670.563333\n",
      "bce: 101.914564, kld: 34.274596\n",
      "Train Epoch: 3 [1500/2307 (71%)]      Loss: 8509.485833\n",
      "bce: 101.461973, kld: 33.632096\n",
      "====> Epoch: 3 Average loss: 7580.2015, bce: 92.5335, kld: 29.9507\n",
      "====> Testing Average Loss: 78.87039275845254\n",
      "Train Epoch: 4 [   0/2307 ( 0%)]      Loss: 7903.827500\n",
      "bce: 101.168743, kld: 31.210638\n",
      "Train Epoch: 4 [1500/2307 (71%)]      Loss: 7482.347500\n",
      "bce: 100.111133, kld: 29.528945\n",
      "====> Epoch: 4 Average loss: 6977.7047, bce: 91.6331, kld: 27.5443\n",
      "====> Testing Average Loss: 78.11264121423928\n",
      "Train Epoch: 5 [   0/2307 ( 0%)]      Loss: 7282.596667\n",
      "bce: 100.090241, kld: 28.730026\n",
      "Train Epoch: 5 [1500/2307 (71%)]      Loss: 7116.292500\n",
      "bce: 99.187109, kld: 28.068421\n",
      "====> Epoch: 5 Average loss: 6529.5769, bce: 90.7184, kld: 25.7554\n",
      "====> Testing Average Loss: 77.62898244473342\n",
      "Train Epoch: 6 [   0/2307 ( 0%)]      Loss: 6945.415833\n",
      "bce: 99.412708, kld: 27.384014\n",
      "Train Epoch: 6 [1500/2307 (71%)]      Loss: 6699.047917\n",
      "bce: 98.249049, kld: 26.403195\n",
      "====> Epoch: 6 Average loss: 6160.4988, bce: 89.8838, kld: 24.2825\n",
      "====> Testing Average Loss: 76.838180164987\n",
      "Train Epoch: 7 [   0/2307 ( 0%)]      Loss: 6640.445833\n",
      "bce: 98.474277, kld: 26.167887\n",
      "Train Epoch: 7 [1500/2307 (71%)]      Loss: 6386.099583\n",
      "bce: 97.739746, kld: 25.153441\n",
      "====> Epoch: 7 Average loss: 5900.7083, bce: 89.1415, kld: 23.2463\n",
      "====> Testing Average Loss: 76.3316401170351\n",
      "Train Epoch: 8 [   0/2307 ( 0%)]      Loss: 6450.102083\n",
      "bce: 97.037897, kld: 25.412257\n",
      "Train Epoch: 8 [1500/2307 (71%)]      Loss: 6038.552083\n",
      "bce: 97.225488, kld: 23.765306\n",
      "====> Epoch: 8 Average loss: 5653.8354, bce: 88.3555, kld: 22.2619\n",
      "====> Testing Average Loss: 75.62610990328349\n",
      "Train Epoch: 9 [   0/2307 ( 0%)]      Loss: 6116.874167\n",
      "bce: 96.187969, kld: 24.082746\n",
      "Train Epoch: 9 [1500/2307 (71%)]      Loss: 5916.467083\n",
      "bce: 96.121445, kld: 23.281383\n",
      "====> Epoch: 9 Average loss: 5463.7824, bce: 87.6290, kld: 21.5046\n",
      "====> Testing Average Loss: 75.07551152064369\n",
      "Train Epoch: 10 [   0/2307 ( 0%)]      Loss: 5848.147917\n",
      "bce: 95.676191, kld: 23.009886\n",
      "Train Epoch: 10 [1500/2307 (71%)]      Loss: 5699.127500\n",
      "bce: 95.650762, kld: 22.413906\n",
      "====> Epoch: 10 Average loss: 5246.3260, bce: 87.0323, kld: 20.6372\n",
      "====> Testing Average Loss: 74.59496149626138\n",
      "Train Epoch: 11 [   0/2307 ( 0%)]      Loss: 5670.311667\n",
      "bce: 94.726354, kld: 22.302342\n",
      "Train Epoch: 11 [1500/2307 (71%)]      Loss: 5395.419167\n",
      "bce: 94.970560, kld: 21.201795\n",
      "====> Epoch: 11 Average loss: 5038.7491, bce: 86.4884, kld: 19.8090\n",
      "====> Testing Average Loss: 73.97885596147594\n",
      "Train Epoch: 12 [   0/2307 ( 0%)]      Loss: 5469.550000\n",
      "bce: 94.759714, kld: 21.499162\n",
      "Train Epoch: 12 [1500/2307 (71%)]      Loss: 5294.558333\n",
      "bce: 94.627363, kld: 20.799723\n",
      "====> Epoch: 12 Average loss: 4847.9521, bce: 85.9796, kld: 19.0479\n",
      "====> Testing Average Loss: 73.5609278486671\n",
      "Train Epoch: 13 [   0/2307 ( 0%)]      Loss: 5186.810833\n",
      "bce: 94.358444, kld: 20.369810\n",
      "Train Epoch: 13 [1500/2307 (71%)]      Loss: 5086.376250\n",
      "bce: 93.445358, kld: 19.971724\n",
      "====> Epoch: 13 Average loss: 4673.1847, bce: 85.4715, kld: 18.3509\n",
      "====> Testing Average Loss: 72.99023691482445\n",
      "Train Epoch: 14 [   0/2307 ( 0%)]      Loss: 5082.197500\n",
      "bce: 93.253210, kld: 19.955776\n",
      "Train Epoch: 14 [1500/2307 (71%)]      Loss: 4787.594167\n",
      "bce: 93.277689, kld: 18.777267\n",
      "====> Epoch: 14 Average loss: 4490.6547, bce: 84.9966, kld: 17.6226\n",
      "====> Testing Average Loss: 72.62870052421977\n",
      "Train Epoch: 15 [   0/2307 ( 0%)]      Loss: 4836.045000\n",
      "bce: 92.843320, kld: 18.972806\n",
      "Train Epoch: 15 [1500/2307 (71%)]      Loss: 4652.371667\n",
      "bce: 92.764798, kld: 18.238426\n",
      "====> Epoch: 15 Average loss: 4297.1407, bce: 84.5038, kld: 16.8505\n",
      "====> Testing Average Loss: 72.05642220009753\n",
      "Train Epoch: 16 [   0/2307 ( 0%)]      Loss: 4616.919583\n",
      "bce: 92.837025, kld: 18.096330\n",
      "Train Epoch: 16 [1500/2307 (71%)]      Loss: 4447.289167\n",
      "bce: 91.922318, kld: 17.421466\n",
      "====> Epoch: 16 Average loss: 4120.1258, bce: 84.0147, kld: 16.1444\n",
      "====> Testing Average Loss: 71.53150398244473\n",
      "Train Epoch: 17 [   0/2307 ( 0%)]      Loss: 4405.627083\n",
      "bce: 92.093275, kld: 17.254136\n",
      "Train Epoch: 17 [1500/2307 (71%)]      Loss: 4262.486667\n",
      "bce: 91.632585, kld: 16.683416\n",
      "====> Epoch: 17 Average loss: 3933.3937, bce: 83.5636, kld: 15.3993\n",
      "====> Testing Average Loss: 71.05742289092979\n",
      "Train Epoch: 18 [   0/2307 ( 0%)]      Loss: 4230.987500\n",
      "bce: 91.397422, kld: 16.558359\n",
      "Train Epoch: 18 [1500/2307 (71%)]      Loss: 4082.353750\n",
      "bce: 90.978503, kld: 15.965501\n",
      "====> Epoch: 18 Average loss: 3756.4693, bce: 83.0505, kld: 14.6937\n",
      "====> Testing Average Loss: 70.48665830217816\n",
      "Train Epoch: 19 [   0/2307 ( 0%)]      Loss: 3997.452500\n",
      "bce: 91.183242, kld: 15.625076\n",
      "Train Epoch: 19 [1500/2307 (71%)]      Loss: 3891.156667\n",
      "bce: 90.467708, kld: 15.202757\n",
      "====> Epoch: 19 Average loss: 3583.2081, bce: 82.4977, kld: 14.0028\n",
      "====> Testing Average Loss: 69.85990836313394\n",
      "Train Epoch: 20 [   0/2307 ( 0%)]      Loss: 3846.413333\n",
      "bce: 90.402135, kld: 15.024045\n",
      "Train Epoch: 20 [1500/2307 (71%)]      Loss: 3680.269167\n",
      "bce: 90.065625, kld: 14.360814\n",
      "====> Epoch: 20 Average loss: 3401.0906, bce: 82.0143, kld: 13.2763\n",
      "====> Testing Average Loss: 69.25323827617035\n",
      "Train Epoch: 21 [   0/2307 ( 0%)]      Loss: 3616.962500\n",
      "bce: 89.728477, kld: 14.108937\n",
      "Train Epoch: 21 [1500/2307 (71%)]      Loss: 3571.954583\n",
      "bce: 89.524466, kld: 13.929720\n",
      "====> Epoch: 21 Average loss: 3235.6941, bce: 81.4706, kld: 12.6169\n",
      "====> Testing Average Loss: 68.58110421407672\n",
      "Train Epoch: 22 [   0/2307 ( 0%)]      Loss: 3452.437500\n",
      "bce: 89.870319, kld: 13.450269\n",
      "Train Epoch: 22 [1500/2307 (71%)]      Loss: 3336.102500\n",
      "bce: 88.984290, kld: 12.988472\n",
      "====> Epoch: 22 Average loss: 3068.3762, bce: 80.9061, kld: 11.9499\n",
      "====> Testing Average Loss: 67.90411654746424\n",
      "Train Epoch: 23 [   0/2307 ( 0%)]      Loss: 3315.273958\n",
      "bce: 88.577109, kld: 12.906787\n",
      "Train Epoch: 23 [1500/2307 (71%)]      Loss: 3142.940208\n",
      "bce: 88.262624, kld: 12.218710\n",
      "====> Epoch: 23 Average loss: 2913.3334, bce: 80.4316, kld: 11.3316\n",
      "====> Testing Average Loss: 67.1709682826723\n",
      "Train Epoch: 24 [   0/2307 ( 0%)]      Loss: 3099.593542\n",
      "bce: 88.166934, kld: 12.045706\n",
      "Train Epoch: 24 [1500/2307 (71%)]      Loss: 3001.162083\n",
      "bce: 87.997435, kld: 11.652659\n",
      "====> Epoch: 24 Average loss: 2749.4008, bce: 79.9514, kld: 10.6778\n",
      "====> Testing Average Loss: 66.4925252966515\n",
      "Train Epoch: 25 [   0/2307 ( 0%)]      Loss: 2919.011875\n",
      "bce: 87.204388, kld: 11.327230\n",
      "Train Epoch: 25 [1500/2307 (71%)]      Loss: 2786.559375\n",
      "bce: 87.063548, kld: 10.797983\n",
      "====> Epoch: 25 Average loss: 2601.7879, bce: 79.3595, kld: 10.0897\n",
      "====> Testing Average Loss: 65.87438028283485\n",
      "Train Epoch: 26 [   0/2307 ( 0%)]      Loss: 2780.278333\n",
      "bce: 86.989329, kld: 10.773156\n",
      "Train Epoch: 26 [1500/2307 (71%)]      Loss: 2675.737083\n",
      "bce: 86.296914, kld: 10.357761\n",
      "====> Epoch: 26 Average loss: 2462.2695, bce: 78.8935, kld: 9.5335\n",
      "====> Testing Average Loss: 65.23158627275683\n",
      "Train Epoch: 27 [   0/2307 ( 0%)]      Loss: 2613.885417\n",
      "bce: 86.405143, kld: 10.109921\n",
      "Train Epoch: 27 [1500/2307 (71%)]      Loss: 2512.837500\n",
      "bce: 86.386862, kld: 9.705802\n",
      "====> Epoch: 27 Average loss: 2328.0079, bce: 78.3634, kld: 8.9986\n",
      "====> Testing Average Loss: 64.66820749349804\n",
      "Train Epoch: 28 [   0/2307 ( 0%)]      Loss: 2469.150000\n",
      "bce: 86.203171, kld: 9.531787\n",
      "Train Epoch: 28 [1500/2307 (71%)]      Loss: 2368.938542\n",
      "bce: 84.796120, kld: 9.136570\n",
      "====> Epoch: 28 Average loss: 2198.5729, bce: 77.9557, kld: 8.4825\n",
      "====> Testing Average Loss: 64.08344339239272\n",
      "Train Epoch: 29 [   0/2307 ( 0%)]      Loss: 2336.569375\n",
      "bce: 85.628014, kld: 9.003765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 [1500/2307 (71%)]      Loss: 2244.110833\n",
      "bce: 84.898750, kld: 8.636848\n",
      "====> Epoch: 29 Average loss: 2081.0519, bce: 77.4581, kld: 8.0144\n",
      "====> Testing Average Loss: 63.54393642311443\n",
      "Train Epoch: 30 [   0/2307 ( 0%)]      Loss: 2226.659792\n",
      "bce: 84.453359, kld: 8.568826\n",
      "Train Epoch: 30 [1500/2307 (71%)]      Loss: 2131.666667\n",
      "bce: 84.443802, kld: 8.188892\n",
      "====> Epoch: 30 Average loss: 1966.8672, bce: 76.9229, kld: 7.5598\n",
      "====> Testing Average Loss: 63.05266579973992\n",
      "Train Epoch: 31 [   0/2307 ( 0%)]      Loss: 2099.896875\n",
      "bce: 83.701387, kld: 8.064782\n",
      "Train Epoch: 31 [1500/2307 (71%)]      Loss: 2009.853333\n",
      "bce: 84.604245, kld: 7.700997\n",
      "====> Epoch: 31 Average loss: 1862.4801, bce: 76.5774, kld: 7.1436\n",
      "====> Testing Average Loss: 62.54371291856307\n",
      "Train Epoch: 32 [   0/2307 ( 0%)]      Loss: 1965.845417\n",
      "bce: 84.037865, kld: 7.527230\n",
      "Train Epoch: 32 [1500/2307 (71%)]      Loss: 1899.808125\n",
      "bce: 83.367012, kld: 7.265764\n",
      "====> Epoch: 32 Average loss: 1762.2786, bce: 76.0396, kld: 6.7450\n",
      "====> Testing Average Loss: 62.06184980494148\n",
      "Train Epoch: 33 [   0/2307 ( 0%)]      Loss: 1871.808542\n",
      "bce: 83.724310, kld: 7.152336\n",
      "Train Epoch: 33 [1500/2307 (71%)]      Loss: 1804.198542\n",
      "bce: 82.737376, kld: 6.885845\n",
      "====> Epoch: 33 Average loss: 1671.0405, bce: 75.6614, kld: 6.3815\n",
      "====> Testing Average Loss: 61.603419111671\n",
      "Train Epoch: 34 [   0/2307 ( 0%)]      Loss: 1778.007917\n",
      "bce: 82.865905, kld: 6.780568\n",
      "Train Epoch: 34 [1500/2307 (71%)]      Loss: 1727.241875\n",
      "bce: 82.524115, kld: 6.578871\n",
      "====> Epoch: 34 Average loss: 1584.1295, bce: 75.2313, kld: 6.0356\n",
      "====> Testing Average Loss: 61.12601592977894\n",
      "Train Epoch: 35 [   0/2307 ( 0%)]      Loss: 1679.053542\n",
      "bce: 82.945065, kld: 6.384434\n",
      "Train Epoch: 35 [1500/2307 (71%)]      Loss: 1622.025000\n",
      "bce: 81.802865, kld: 6.160889\n",
      "====> Epoch: 35 Average loss: 1503.0441, bce: 74.8405, kld: 5.7128\n",
      "====> Testing Average Loss: 60.72978553722367\n",
      "Train Epoch: 36 [   0/2307 ( 0%)]      Loss: 1615.976042\n",
      "bce: 81.670866, kld: 6.137221\n",
      "Train Epoch: 36 [1500/2307 (71%)]      Loss: 1539.307083\n",
      "bce: 81.974206, kld: 5.829332\n",
      "====> Epoch: 36 Average loss: 1429.7730, bce: 74.4347, kld: 5.4214\n",
      "====> Testing Average Loss: 60.24601247561768\n",
      "Train Epoch: 37 [   0/2307 ( 0%)]      Loss: 1530.904583\n",
      "bce: 82.078197, kld: 5.795306\n",
      "Train Epoch: 37 [1500/2307 (71%)]      Loss: 1474.786667\n",
      "bce: 80.867630, kld: 5.575676\n",
      "====> Epoch: 37 Average loss: 1360.2935, bce: 74.0759, kld: 5.1449\n",
      "====> Testing Average Loss: 59.84663778039662\n",
      "Train Epoch: 38 [   0/2307 ( 0%)]      Loss: 1448.769583\n",
      "bce: 81.300306, kld: 5.469877\n",
      "Train Epoch: 38 [1500/2307 (71%)]      Loss: 1395.311042\n",
      "bce: 79.837852, kld: 5.261893\n",
      "====> Epoch: 38 Average loss: 1294.4459, bce: 73.7069, kld: 4.8830\n",
      "====> Testing Average Loss: 59.42529614353056\n",
      "Train Epoch: 39 [   0/2307 ( 0%)]      Loss: 1386.548646\n",
      "bce: 80.879212, kld: 5.222678\n",
      "Train Epoch: 39 [1500/2307 (71%)]      Loss: 1318.203229\n",
      "bce: 79.447077, kld: 4.955024\n",
      "====> Epoch: 39 Average loss: 1231.4624, bce: 73.3722, kld: 4.6324\n",
      "====> Testing Average Loss: 59.02854508696359\n",
      "Train Epoch: 40 [   0/2307 ( 0%)]      Loss: 1312.301875\n",
      "bce: 80.075977, kld: 4.928904\n",
      "Train Epoch: 40 [1500/2307 (71%)]      Loss: 1276.393125\n",
      "bce: 79.691510, kld: 4.786807\n",
      "====> Epoch: 40 Average loss: 1174.0102, bce: 72.9319, kld: 4.4043\n",
      "====> Testing Average Loss: 58.65717195627438\n",
      "Train Epoch: 41 [   0/2307 ( 0%)]      Loss: 1252.592396\n",
      "bce: 80.237663, kld: 4.689419\n"
     ]
    }
   ],
   "source": [
    "local_dataset='/home/ftamagnan/dataset/FillsExtractedClustering.npz'\n",
    "\n",
    "tg=TrainingSketchRnn(lr=LR,batch_size=BATCH_SIZE,n_epochs=N_EPOCHS,dataset_filepath=local_dataset,beta=250,linear_hidden_size=[64,32],gru_hidden_size=64)\n",
    "tg.load_data()\n",
    "tg.split_data()\n",
    "tg.train_model()\n",
    "tg.save_model(\"./../models/\",'sketchrnn_clustering.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
